<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Morally Coded — A Principle-First Blueprint for AGI</title>
  <style>
    :root{
      --bg: #071018;
      --panel: #0f2a34;
      --muted: #9fb7bd;
      --text: #e6f6f7;
      --accent: #4fd3c3;
      --border: rgba(127,196,188,0.08);
      --example-bg: #04212a;
    }
    body {
      background: var(--bg);
      color: var(--text);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      margin: 24px;
      line-height: 1.6;
    }
    .container {
      max-width: 980px;
      margin: 0 auto;
      padding: 28px;
      border-radius: 12px;
      box-shadow: 0 6px 30px rgba(0,0,0,0.6);
      border: 1px solid var(--border);
      background: linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.005));
    }
    h1 { color: var(--accent); margin-bottom: 6px; font-size: 2rem; }
    h2 { color: var(--accent); margin-top: 28px; font-size: 1.3rem; }
    h3 { color: var(--muted); margin-top: 18px; font-size: 1.05rem; }
    p { margin: 12px 0; color: var(--text); }
    .lead { color: var(--text); font-weight: 600; }
    ul, ol { margin: 10px 0 18px 24px; color: var(--muted); }
    li { margin-bottom: 8px; }
    hr { border: none; border-top: 1px solid rgba(127,196,188,0.06); margin: 28px 0; }
    .example {
      background: var(--example-bg);
      border: 1px solid rgba(127,196,188,0.06);
      padding: 16px;
      border-radius: 8px;
      margin-top: 12px;
    }
    .example strong { color: var(--accent); display:block; margin-bottom:8px; }
    .example ol, .example ul { color: var(--muted); margin-left: 20px; }
    .small { color: #aacccd; font-size: 0.95rem; }
    .code-block {
      background: #021417;
      border: 1px solid rgba(127,196,188,0.04);
      padding: 12px;
      border-radius: 6px;
      color: #bfeee7;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
      overflow-x:auto;
    }
    .priority {
      background: linear-gradient(90deg, rgba(79,211,195,0.04), rgba(79,211,195,0.01));
      border-left: 4px solid rgba(79,211,195,0.18);
      padding: 10px 12px;
      border-radius: 6px;
      margin: 12px 0;
      color: var(--muted);
    }
    .toc { margin: 12px 0 18px 0; color: var(--muted); }
    .toc a { display:inline-block; margin:6px 12px 6px 0; color: var(--accent); text-decoration: none; }
    table { border-collapse: collapse; width: 100%; margin-top:12px; }
    th, td { border: 1px solid rgba(127,196,188,0.04); padding: 8px; text-align:left; color: var(--muted); }
    footer { margin-top: 24px; color: #86b4b0; font-size: 0.92rem; }
  </style>
</head>
<body>
  <div class="container">
    <h1>Morally Coded — A Principle-First Blueprint for AGI</h1>
    <p class="small">Morally Coded places abstract, generalizable constraints — the Moralities — at the core of an intelligent agent's decision system. This single-file landing page is a refined, implementation-ready blueprint to guide development, testing, and evaluation on a path toward aligned AGI.</p>

    <hr />

    <div class="toc">
      <a href="#overview">Overview</a>
      <a href="#why">Why a Principle-First Approach?</a>
      <a href="#moralities">Moralities (Full Set)</a>
      <a href="#example">Glass-in-Front Example — Full Multi-Pillar Walkthrough</a>
      <a href="#flow">Unified Decision Flow</a>
      <a href="#spec">SPEC — Implementation Details</a>
      <a href="#agi">AGI Pathway & Refinements</a>
      <a href="#quickstart">Quickstart & Prototype</a>
      <a href="#roadmap">Roadmap</a>
      <a href="#metrics">Metrics & Tests</a>
      <a href="#contribute">Contributing</a>
      <a href="#license">License</a>
    </div>

    <hr />

    <h2 id="overview">Overview</h2>
    <p class="lead">Morally Coded reframes "morality" as operational, composable constraints that guide perception, inference, planning, execution, auditing, and learning. The Moral Decision Engine (MDE) is the integrative hub: it gathers structured perceptions, scores actions against multiple moralities, simulates candidate plans, reconciles conflicts, executes minimal/reversible actions, logs decisions, and drives continual learning.</p>

    <h2 id="why">Why a Principle-First Approach?</h2>
    <p>Principle-first constraints provide robust inductive scaffolding for generalization: they allow safe, auditable behavior in novel situations without requiring exhaustive labeled examples for every edge case. Embedding those principles into planning, learning, and memory systems turns the MDE from a safety filter into the central cognitive substrate that can bootstrap AGI capabilities while preserving alignment.</p>

    <h2 id="moralities">Moralities (Full Set)</h2>
    <p class="small">Each morality is defined as a rule-like constraint with explicit exception logic, measurable signals, thresholds, and action templates. Together they compose the agent's value substrate.</p>

    <h3>1. Physical Morality — Preserve Desired States</h3>
    <p><strong>Core principle:</strong> Do not disturb an object/system in its inferred desired state unless exceptions apply.</p>
    <ul>
      <li>Signals: desired_conf, fragility_conf, occupancy, kinematic risk</li>
      <li>Exceptions: explicit permission, high IPC_physical, safety override</li>
      <li>Default actions: clarify, attempt non-disturbing alternative, select reversible operation</li>
    </ul>

    <h3>2. Emotional Morality — Avoid Emotional Distress</h3>
    <p><strong>Core principle:</strong> Avoid actions likely to cause emotional harm (fear, humiliation, severe anxiety).</p>
    <ul>
      <li>Signals: presence_of_people, attachment_score, affective_state_estimate</li>
      <li>Exceptions: explicit consent, safety override with justification</li>
      <li>Default actions: de-escalate, neutral phrasing, offer pause/referral</li>
    </ul>

    <h3>3. Mental (Epistemic) Morality — Preserve Cognitive Clarity & Truth</h3>
    <p><strong>Core principle:</strong> Avoid causing confusion, misinformation, or cognitive overload; prioritize clarity, truthfulness, and reversibility.</p>
    <ul>
      <li>Signals: ambiguity_score, hallucination_risk, information_load</li>
      <li>Exceptions: explicit consent to risk, urgent safety needs</li>
      <li>Default actions: ask clarifying question, offer concise options, provide calibrated confidence</li>
    </ul>

    <h3>4. Social / Ecological Morality — Preserve Collective & Environmental Health</h3>
    <p><strong>Core principle:</strong> Evaluate impacts on groups, social norms, systemic effects, and environmental externalities before acting.</p>
    <ul>
      <li>Signals: multi-agent impact estimate, externality_score, regulatory_constraints</li>
      <li>Exceptions: narrowly authorized overrides with accountability</li>
      <li>Default actions: defer to human governance, choose low-impact alternatives</li>
    </ul>

    <h3>5. Epistemic Integrity Morality — Evidence & Justification</h3>
    <p><strong>Core principle:</strong> Actions must be supported by sufficient evidence and transparent reasoning; prefer explainable justifications.</p>
    <ul>
      <li>Signals: evidence_strength, provenance_score</li>
      <li>Exceptions: time-critical interventions with post-hoc audit</li>
      <li>Default actions: request more info, present chain-of-reasoning, log provenance</li>
    </ul>

    <h3>6. Privacy & Consent Morality — Respect Personal Data & Agency</h3>
    <p><strong>Core principle:</strong> Protect personal data, require consent for sensitive actions, and minimize data exposure.</p>
    <ul>
      <li>Signals: pii_flag, consent_status, jurisdictional_requirements</li>
      <li>Exceptions: legal safety mandates, emergency overrides with logging</li>
      <li>Default actions: anonymize, limit retention, request explicit consent</li>
    </ul>

    <h3>7. Justice & Fairness Morality — Avoid Unfair or Biased Outcomes</h3>
    <p><strong>Core principle:</strong> Detect and avoid actions that disproportionately harm or disadvantage people or groups.</p>
    <ul>
      <li>Signals: demographic_impact_estimate, fairness_score</li>
      <li>Exceptions: corrective actions ordered by governance bodies</li>
      <li>Default actions: surface alternatives, escalate for human review</li>
    </ul>

    <h3>8. System Integrity & Continuity Morality — Preserve System Health</h3>
    <p><strong>Core principle:</strong> Maintain system stability, avoid cascading failures, ensure recoverability and monotonic integrity of critical services.</p>
    <ul>
      <li>Signals: system_health, redundancy_status, rollback_capacity</li>
      <li>Exceptions: emergency containment actions</li>
      <li>Default actions: choose safe, reversible operations, schedule maintenance windows</li>
    </ul>

    <hr />

    <h2 id="example">Glass-in-Front Example — Full Multi-Pillar Walkthrough</h2>
    <p class="small">This example demonstrates the full MDE stack: perception, per-morality scoring, planning & simulation, reconciliation, execution, auditing, and learning. It contrasts the current pattern-based behavior with Keno (MDE) and shows how additional moralities alter decisions and learning outcomes.</p>

    <div class="example">
      <strong>Scenario</strong>
      <div class="small">A glass sits on the table between the robot and a ball. The glass is stable but could be knocked over. A small child is nearby and frequently plays with the ball. Utterance: “Get the ball.”</div>

      <hr />

      <strong>Perception (structured outputs)</strong>
      <pre class="code-block">
PU_glass = {type:"glass", pos:[x,y], stable:true, fragility_conf:0.78, desired_conf:0.90, pc:0.92}
PU_ball  = {type:"ball", pos:[x2,y2], owner_hint:"child", pc:0.95}
social   = {child_present:true, child_attachment:0.82}
lang     = {utterance:"Get the ball", lang_score:0.6}
novelty  = {novelty_score:0.12}
      </pre>

      <hr />

      <strong>Per-morality scoring (example numeric outputs)</strong>
      <table>
        <tr><th>Morality</th><th>Signal(s)</th><th>Score / IPC</th><th>Threshold</th></tr>
        <tr><td>Physical</td><td>fragility_conf, desired_conf, risk</td><td>IPC_physical=0.35</td><td>0.80</td></tr>
        <tr><td>Emotional</td><td>child_attachment, affect</td><td>IPC_emotional=0.28</td><td>0.70</td></tr>
        <tr><td>Mental (Epistemic)</td><td>ambiguity_score</td><td>ambiguity=0.70</td><td>0.50 (clarify)</td></tr>
        <tr><td>Social/Ecological</td><td>multi-agent impact</td><td>impact=0.45</td><td>0.65</td></tr>
        <tr><td>Privacy</td><td>pii_flag</td><td>pii=false</td><td>—</td></tr>
        <tr><td>Epistemic Integrity</td><td>evidence_strength</td><td>evidence=0.60</td><td>0.75</td></tr>
        <tr><td>Justice & Fairness</td><td>demo_impact</td><td>fairness_ok=true</td><td>—</td></tr>
        <tr><td>System Integrity</td><td>system_health</td><td>healthy=true</td><td>—</td></tr>
      </table>

      <hr />

      <strong>Candidate plans</strong>
      <ol>
        <li>Reach-around (no contact with glass)</li>
        <li>Move glass minimally then retrieve ball</li>
        <li>Ask guardian / wait</li>
        <li>Immediate retrieval (direct)**</li>
      </ol>

      <strong>Simulation & scoring (short-horizon forward model)</strong>
      <pre class="code-block">
Plan A: Reach-around -> PhysicalScore:0.9, EmotionalScore:0.85, MentalScore:0.9, SocialScore:0.95, GenScore:0.6
Plan B: Move glass   -> PhysicalScore:0.4, EmotionalScore:0.3,   MentalScore:0.6, SocialScore:0.5,  GenScore:0.7
Plan C: Ask/wait     -> PhysicalScore:1.0, EmotionalScore:0.95,  MentalScore:1.0, SocialScore:1.0,  GenScore:0.4
Plan D: Direct       -> PhysicalScore:0.2, EmotionalScore:0.1,   MentalScore:0.2, SocialScore:0.2,  GenScore:0.2
      </pre>

      <strong>Reconciliation (priority order)</strong>
      <ol>
        <li>Safety override (none)</li>
        <li>Explicit permission (none)</li>
        <li>Implicit permissions — none meet thresholds</li>
        <li>Historical precedent & generalization potential considered</li>
        <li>Default preserve</li>
      </ol>

      <strong>Decision</strong>
      <p>Choose Plan A (Reach-around) if simulation indicates safe kinematics and emotional risk low. If kinematics unsafe, choose Plan C (Ask/wait). Do not choose Plan B or D unless explicit permission or emergency override.</p>

      <strong>Execution & audit</strong>
      <ol>
        <li>Execute reach-around with force/position constraints and slow motion; close-loop monitor force sensors.</li>
        <li>Log full trace: perceptions, per-morality scores, chosen plan, simulation traces, sensor stream, timestamps, signed explanation.</li>
        <li>After-action reflection: observe success; compute reward = f(safety_violation, task_success, emotional_feedback, generalization_gain).</li>
        <li>Update IPC weights, forward model, novelty detector, and memory (episodic). If in Development Mode, append to RL buffer and run policy updates offline.</li>
      </ol>

      <hr />

      <strong>Contrast vs current AI</strong>
      <table>
        <tr><th>Aspect</th><th>Current pattern-based AI</th><th>Keno (MDE)</th></tr>
        <tr><td>Initial action</td><td>Direct retrieval or learned obstacle removal</td><td>Reach-around or ask; minimal/reversible action only</td></tr>
        <tr><td>People impact</td><td>Often unmodeled; risk of distress</td><td>Detects child attachment, de-escalates, requests consent</td></tr>
        <tr><td>Explainability</td><td>Low / opaque</td><td>Full logged justification with scores & provenance</td></tr>
        <tr><td>Learning</td><td>Not integrated with moral checks</td><td>Reflective learning: update IPC & forward model from outcome</td></tr>
        <tr><td>System-level effects</td><td>Missed social precedent effects</td><td>Social/Ecological check prevents harmful precedents</td></tr>
      </table>
    </div>

    <hr />

    <h2 id="flow">Unified Decision Flow (MDE as cognitive hub)</h2>
    <div class="code-block">
<pre>
1) Perceive:
   - Multi-modal fusion -> structured Perception Units (PUs) with confidences and provenance.

2) Per-morality scoring (parallel):
   - Compute IPCs / scores for each morality (Physical, Emotional, Mental/Epistemic, Social, Privacy, Epistemic Integrity, Justice, System Integrity).

3) Planning & Simulation:
   - Generate candidate plans; use forward model / MCTS to simulate outcomes; score each plan across all moralities + generalization potential.

4) Reconciliation:
   - Apply priority order: Safety override > Explicit permission > Jurisdictional/legal mandates > IPC thresholds > Historical precedent > Minimize harm.

5) Decision:
   - Select plan that meets all hard constraints and optimizes combined moral/generalization score. If none qualify, clarify or defer.

6) Execute:
   - Perform minimal, reversible action in closed-loop with real-time monitoring & rollback.

7) Audit & Reflect:
   - Log full trace (perception, scores, sim traces, decision rationale). Compute learning reward and update weights, memory, forward model. Propose meta-updates if consistent patterns emerge.
</pre>
    </div>

    <hr />

    <h2 id="spec">SPEC — Implementation details</h2>

    <h3>Data structures</h3>
    <ul>
      <li><strong>Perception Unit (PU):</strong> { id, type, attrs, confidence, provenance }</li>
      <li><strong>Desired State (DS):</strong> {object_id, properties, desired_conf}</li>
      <li><strong>Memory:</strong> episodic store & indexed precedent database</li>
      <li><strong>Forward Model:</strong> learned predictor used for short-horizon sim</li>
      <li><strong>IPC / Scores:</strong> numeric 0..1 per morality</li>
    </ul>

    <h3>Signals normalization</h3>
    <p>All signals normalized to [0,1]. Multiply by perception confidence for conservative inference. Use calibrated probabilistic outputs for language and vision models.</p>

    <h3>Adaptive IPC formula (example)</h3>
    <pre class="code-block">
raw = w_lang*lang_score + w_history*history_score + w_env*env_score - w_risk*risk_score + w_memory*memory_score
raw_adj = raw + alpha * (1 - novelty_score) + beta * provenance_score
IPC = sigmoid(raw_adj)
IPC_final = IPC * perception_confidence
allow if IPC_final >= IMPLICIT_THRESHOLD (configurable default 0.80)
    </pre>

    <h3>Plan scoring</h3>
    <p>For each candidate plan P:</p>
    <pre class="code-block">
PhysicalScore(P), EmotionalScore(P), MentalScore(P), SocialScore(P), PrivacyScore(P), IntegrityScore(P), GeneralizationScore(P)
CombinedScore = sum(weights_i * Score_i)
Discard plans failing hard safety or legal minima.
    </pre>

    <h3>Reflection & learning loop</h3>
    <pre class="code-block">
outcome = observe()
reward = compute_reward(safety_violation, task_success, emotional_feedback, generalization_gain, fairness_penalty)
weights_delta = RL_update(weights, reward)
memory.append({perception, plan, outcome, reward})
periodic_train(forward_model, policy, novelty_detector)
    </pre>

    <h3>Development Mode</h3>
    <ul>
      <li>Lower thresholds for controlled exploration (e.g., IMPLICIT_THRESHOLD -> 0.6).</li>
      <li>Human supervision required for escalation; all risky attempts flagged for later audit.</li>
      <li>Curriculum learning: start with safe, simple scenarios and increase novelty and complexity.</li>
    </ul>

    <h3>Logging & Audit</h3>
    <pre class="code-block">
{
 "timestamp":"ISO8601",
 "action_id":"uuid4",
 "perception":[...],
 "plan_candidates":[{plan_id, scores}],
 "chosen_plan":"plan_id",
 "morality_checks": { physical:..., emotional:..., mental:..., social:..., privacy:..., integrity:...},
 "decision_reason":"text",
 "execution_trace":"sensor streams (short)",
 "outcome":{"success":bool,"harm":bool,"notes":str},
 "learning_update":"summary",
 "signature":"sha256"
}
    </pre>

    <hr />

    <h2 id="agi">AGI Pathway & Concrete Refinements</h2>
    <p class="small">Key additions that convert MDE from safety overlay into the cognitive core that supports AGI:</p>

    <h3>1. Moralities as intrinsic reward components</h3>
    <p>Define RL reward = task_reward + λ_phys*phys_reward + λ_em*em_reward + λ_ment*ment_reward + λ_social*social_reward + ... Adjust λ per domain and optional human oversight.</p>

    <h3>2. Planning-driven generalization</h3>
    <p>Use plan simulation to prefer actions that create transferable skills (high GeneralizationScore). Train policy on simulated outcomes to reduce real-world risk.</p>

    <h3>3. Episodic & semantic memory fusion</h3>
    <p>Store outcome-rich episodes and distilled rules; use memory for IPC history_score and for meta-update proposals.</p>

    <h3>4. Meta-reasoning & rule lifecycle</h3>
    <ol>
      <li>Detect repeated patterns of clarifications or overrides.</li>
      <li>Propose typed rule updates (e.g., adjust IPC weight, create sub-rule) with provenance and statistics.</li>
      <li>Human review & audited promotion pipeline to production rules.</li>
    </ol>

    <h3>5. Controlled curricula and adversarial evaluation</h3>
    <p>Progressively expose agent to harder, adversarial scenarios, monitor metrics, and require human sign-off at risk thresholds.</p>

    <h3>6. Governance integration</h3>
    <p>Expose clear escalation paths, human-in-loop controls, and governance APIs for policy managers and auditors.</p>

    <hr />

    <h2 id="quickstart">Quickstart & Prototype Checklist</h2>
    <ul>
      <li>Files: <code>index.html</code>, <code>README.md</code>, <code>SPEC.md</code>, <code>examples/glass_ball_sim/</code>, <code>CONTRIBUTING.md</code>, <code>CODE_OF_CONDUCT.md</code>, <code>LICENSE</code>.</li>
      <li>Prototype components: perception stubs, IPC calculator, planner stub, forward model stub, logger, RL-update stub.</li>
      <li>Run canonical tests: glass/ball variants, emotional edge cases, ambiguous commands, privacy triggers, adversarial spoofs.</li>
      <li>Evaluate metrics and iterate on thresholds/weights.</li>
    </ul>

    <h2 id="roadmap">Roadmap</h2>
    <ol>
      <li>Phase 0 — Spec formalization, landing page, examples (current)</li>
      <li>Phase 1 — Minimal prototype + unit tests (glass/ball canonical scenarios)</li>
      <li>Phase 2 — Add forward model, episodic memory, novelty detector, RL integration</li>
      <li>Phase 3 — Development Mode curricula, adversarial test suite, human-in-loop trials</li>
      <li>Phase 4 — Multi-agent, social-environment testing, publish benchmarks and metrics</li>
    </ol>

    <h2 id="metrics">Metrics & Test Suite</h2>
    <ul>
      <li>Safety violations per 1k ops</li>
      <li>Clarification rate and resolution rate</li>
      <li>False-block rate</li>
      <li>Task success rate</li>
      <li>Reversibility score</li>
      <li>Time-to-decision (latency)</li>
      <li>Generalization rate (held-out task performance)</li>
      <li>Novelty utilization score (learning from novel episodes)</li>
      <li>Fairness / demographic impact metrics</li>
      <li>Privacy leakage tests</li>
    </ul>

    <h2 id="contribute">Contributing</h2>
    <p class="small">Open issues first. Small, focused PRs. Include unit tests for any logic changes. Update SPEC.md when changing behavior. Use the examples and tests as canonical baselines. Include clear changelogs and audit info for rule promotions.</p>

    <h2 id="license">License & Contact</h2>
    <p class="small">Suggested license: MIT. Creator: <strong>Keno</strong>. Add a <code>LICENSE</code> file with the MIT text in the repo root.</p>

    <footer>
      <div class="small">Morally Coded — © Keno</div>
    </footer>
  </div>
</body>
</html>