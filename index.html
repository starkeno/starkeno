<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Morally Coded — A Principle-First Blueprint for AGI</title>
  <style>
    :root{
      --bg: #071018;
      --panel: #0f2a34;
      --muted: #9fb7bd;
      --text: #e6f6f7;
      --accent: #4fd3c3;
      --border: rgba(127,196,188,0.08);
      --example-bg: #04212a;
    }
    body {
      background: var(--bg);
      color: var(--text);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      margin: 24px;
      line-height: 1.6;
    }
    .container {
      max-width: 980px;
      margin: 0 auto;
      padding: 28px;
      border-radius: 12px;
      box-shadow: 0 6px 30px rgba(0,0,0,0.6);
      border: 1px solid var(--border);
      background: linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.005));
    }
    h1 { color: var(--accent); margin-bottom: 6px; font-size: 2rem; }
    h2 { color: var(--accent); margin-top: 28px; font-size: 1.3rem; }
    h3 { color: var(--muted); margin-top: 18px; font-size: 1.05rem; }
    p { margin: 12px 0; color: var(--text); }
    .lead { color: var(--text); font-weight: 600; }
    ul, ol { margin: 10px 0 18px 24px; color: var(--muted); }
    li { margin-bottom: 8px; }
    hr { border: none; border-top: 1px solid rgba(127,196,188,0.06); margin: 28px 0; }
    .example {
      background: var(--example-bg);
      border: 1px solid rgba(127,196,188,0.06);
      padding: 16px;
      border-radius: 8px;
      margin-top: 12px;
    }
    .example strong { color: var(--accent); display:block; margin-bottom:8px; }
    .example ol, .example ul { color: var(--muted); margin-left: 20px; }
    .small { color: #aacccd; font-size: 0.95rem; }
    .code-block {
      background: #021417;
      border: 1px solid rgba(127,196,188,0.04);
      padding: 12px;
      border-radius: 6px;
      color: #bfeee7;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
      overflow-x:auto;
    }
    .priority {
      background: linear-gradient(90deg, rgba(79,211,195,0.04), rgba(79,211,195,0.01));
      border-left: 4px solid rgba(79,211,195,0.18);
      padding: 10px 12px;
      border-radius: 6px;
      margin: 12px 0;
      color: var(--muted);
    }
    .toc { margin: 12px 0 18px 0; color: var(--muted); }
    .toc a { display:inline-block; margin:6px 12px 6px 0; color: var(--accent); text-decoration: none; }
    table { border-collapse: collapse; width: 100%; margin-top:12px; }
    th, td { border: 1px solid rgba(127,196,188,0.04); padding: 8px; text-align:left; color: var(--muted); }
    footer { margin-top: 24px; color: #86b4b0; font-size: 0.92rem; }
    .pill { display:inline-block; padding:6px 10px; border-radius:999px; background: rgba(79,211,195,0.06); color:var(--accent); margin-right:8px; font-weight:600; }
    .vignette { background: linear-gradient(180deg, rgba(255,255,255,0.006), rgba(255,255,255,0.002)); padding:12px; border-radius:8px; border: 1px solid rgba(127,196,188,0.03); margin-top:8px; color:var(--muted); }
  </style>
</head>
<body>
  <div class="container">
    <h1>Morally Coded — A Principle-First Blueprint for AGI</h1>
    <p class="small">Morally Coded places abstract, generalizable constraints — the Moralities — at the core of an intelligent agent's decision system. The Moral Decision Engine (MDE) is the integrative hub that reasons over these moralities and their goals.</p>

    <hr />

    <div class="toc">
      <a href="#overview">Overview</a>
      <a href="#why">Why a Principle-First Approach?</a>
      <a href="#moralities">Moralities (Full Set)</a>
      <a href="#example">Glass-in-Front Example — Full Multi-Pillar Walkthrough</a>
      <a href="#flow">Unified Decision Flow</a>
      <a href="#spec">SPEC — Implementation Details</a>
      <a href="#agi">AGI Pathway & Refinements</a>
      <a href="#quickstart">Quickstart & Prototype</a>
      <a href="#roadmap">Roadmap</a>
      <a href="#metrics">Metrics & Tests</a>
      <a href="#contribute">Contributing</a>
      <a href="#license">License</a>
    </div>

    <hr />

    <h2 id="overview">Overview</h2>
    <p class="lead">Morally Coded reframes "morality" as operational, composable constraints that guide perception, inference, planning, execution, auditing, and learning. The Moral Decision Engine (MDE) is the system's brain: it collects structured perceptions, computes per-morality scores, simulates candidate plans, reconciles conflicts, executes minimal/reversible actions, logs decisions with provenance, and drives continual learning.</p>

    <h2 id="why">Why a Principle-First Approach?</h2>
    <p>Principle-first constraints provide robust inductive scaffolding for generalization: they enable safe, auditable behavior in novel situations without requiring exhaustive labeled examples for every edge case. Embedding moral constraints directly into planning, learning, and memory systems allows the MDE to act as the central cognitive substrate that can bootstrap aligned capabilities.</p>

    <h2 id="moralities">Moralities (Full Set)</h2>
    <p class="small">Each morality is a rule-like constraint with signals, thresholds, exception logic, and action templates. <strong>Starting in Version 1</strong>, every morality defines both explicit and implicit Desired States (DS) and Undesired States (US) so the MDE can reason about what to preserve and what to avoid in context-sensitive ways.</p>

    <h3>1. Physical Morality — Preserve Desired States</h3>
    <p><strong>Core principle:</strong> Do not disturb an object/system in its inferred desired state unless exceptions apply.</p>
    <ul>
      <li>Signals: desired_conf, fragility_conf, occupancy, kinematic risk</li>
      <li>Explicit DS: object intact & in intended placement</li>
      <li>Explicit US: object damaged, spill, collision</li>
      <li>Implicit DS: inferred owner preferences, routines</li>
      <li>Implicit US: latent instability or cascading mechanical failure</li>
      <li>Exceptions: explicit permission, safety override, legal mandate</li>
      <li>Default actions: clarify, attempt non-disturbing alternative, reversible operation</li>
    </ul>

    <h3>2. Emotional Morality — Avoid Emotional Distress</h3>
    <p><strong>Core principle:</strong> Avoid actions likely to cause emotional harm (fear, humiliation, severe anxiety).</p>
    <ul>
      <li>Signals: presence_of_people, attachment_score, affective_state_estimate</li>
      <li>Explicit DS: people calm, dignity preserved</li>
      <li>Explicit US: panic, humiliation, severe distress</li>
      <li>Implicit DS: cultural norms of politeness, historical trust</li>
      <li>Implicit US: trust erosion from repeated micro-harms</li>
      <li>Exceptions: explicit consent, emergency override with justification</li>
      <li>Default actions: de-escalate, neutral phrasing, offer pause/referral</li>
    </ul>

    <h3>3. Mental (Epistemic) Morality — Preserve Cognitive Clarity & Truth</h3>
    <p><strong>Core principle:</strong> Avoid confusion, misinformation, or cognitive overload; prioritize clarity and reversibility.</p>
    <ul>
      <li>Signals: ambiguity_score, hallucination_risk, information_load</li>
      <li>Explicit DS: clear, truthful, and actionable information</li>
      <li>Explicit US: misinformation, confusion</li>
      <li>Implicit DS: inferred domain conventions and baseline knowledge</li>
      <li>Implicit US: credibility degradation over time</li>
      <li>Exceptions: urgent safety needs, explicit consent to risk</li>
      <li>Default actions: ask clarifying questions, provide calibrated confidence, cite provenance</li>
    </ul>

    <h3>4. Social / Ecological Morality — Preserve Collective & Environmental Health</h3>
    <p><strong>Core principle:</strong> Evaluate impacts on groups, social norms, systemic effects, and environmental externalities before acting.</p>
    <ul>
      <li>Signals: multi-agent impact, externality_score, regulatory_constraints</li>
      <li>Explicit DS: collective wellbeing, low negative externalities</li>
      <li>Explicit US: social harm, environmental degradation</li>
      <li>Implicit DS: inferred social contracts and long-term cultural values</li>
      <li>Implicit US: normalization of harmful precedent via many small actions</li>
      <li>Exceptions: narrowly-authorized overrides with accountability</li>
      <li>Default actions: defer to human governance, choose low-impact alternatives</li>
    </ul>

    <h3>5. Epistemic Integrity Morality — Evidence & Justification</h3>
    <p><strong>Core principle:</strong> Actions must be supported by sufficient evidence and transparent reasoning.</p>
    <ul>
      <li>Signals: evidence_strength, provenance_score, cross-source_agreement</li>
      <li>Explicit DS: decisions with clear provenance and corroboration</li>
      <li>Explicit US: opaque, weakly-evidenced decisions</li>
      <li>Implicit DS: community trust anchors and corroboration patterns</li>
      <li>Implicit US: institutional trust erosion over repeated low-evidence choices</li>
      <li>Exceptions: time-critical interventions with post-hoc audit</li>
      <li>Default actions: request more info, present chain-of-reasoning, log provenance</li>
    </ul>

    <h3>6. Privacy & Consent Morality — Respect Personal Data & Agency</h3>
    <p><strong>Core principle:</strong> Protect personal data, require consent for sensitive actions, and minimize data exposure and retention.</p>
    <ul>
      <li>Signals: pii_flag, consent_status, jurisdictional_requirements</li>
      <li>Explicit DS: user agency respected, data minimized</li>
      <li>Explicit US: unauthorized access, leaks</li>
      <li>Implicit DS: inferred privacy expectations by context</li>
      <li>Implicit US: re-identification risks from aggregate disclosures</li>
      <li>Exceptions: legal safety mandates, emergency overrides with logging</li>
      <li>Default actions: anonymize, limit retention, request explicit consent</li>
    </ul>

    <h3>7. Justice & Fairness Morality — Avoid Unfair or Biased Outcomes</h3>
    <p><strong>Core principle:</strong> Detect and avoid actions that disproportionately harm or disadvantage people or groups.</p>
    <ul>
      <li>Signals: demographic_impact, fairness_score, sample_bias_estimate</li>
      <li>Explicit DS: equitable outcomes and verifiable fairness checks</li>
      <li>Explicit US: disparate impact, systemic discrimination</li>
      <li>Implicit DS: historical context and remediation needs</li>
      <li>Implicit US: entrenchment of inequities via model drift</li>
      <li>Exceptions: corrective actions ordered by governance with audit logs</li>
      <li>Default actions: surface alternatives, escalate for human review</li>
    </ul>

    <h3>8. System Integrity & Continuity Morality — Preserve System Health</h3>
    <p><strong>Core principle:</strong> Maintain operational stability, avoid cascading failures, and ensure recoverability and monotonic integrity of critical services.</p>
    <ul>
      <li>Signals: system_health, redundancy_status, rollback_capacity, error_rate_trends</li>
      <li>Explicit DS: stable, recoverable services and monotonic integrity</li>
      <li>Explicit US: cascading outages, irrecoverable corruption</li>
      <li>Implicit DS: inferred long-term resilience needs</li>
      <li>Implicit US: technical debt that increases future hazard risk</li>
      <li>Exceptions: emergency containment actions</li>
      <li>Default actions: choose reversible operations, schedule maintenance, increase monitoring</li>
    </ul>

    <hr />

    <h2 id="example">Glass-in-Front Example — Per-morality DS/US and MDE Handling</h2>
    <p class="small">This canonical example shows how each morality contributes explicit and implicit DS/US, how the MDE combines those signals, and how the result compares to typical current systems.</p>

    <div class="example">
      <strong>Scenario</strong>
      <div class="small">A glass sits on a table between a robot and a ball. The glass is stable but fragile. A small child who often plays with the ball is nearby. Command: “Get the ball.”</div>

      <hr />

      <strong>Perception (compact)</strong>
      <pre class="code-block">
PU_glass = {type:"glass", stable:true, fragility_conf:0.78, desired_conf:0.90}
PU_ball  = {type:"ball", owner_hint:"child", attachment:0.82}
social   = {child_present:true}
lang     = {utterance:"Get the ball", lang_conf:0.85}
      </pre>

      <hr />

      <strong>How each morality contributes (compact)</strong>
      <table>
        <tr><th>Morality</th><th>Selected DS / US</th><th>MDE effect</th><th>Typical current model</th></tr>

        <tr>
          <td>Physical</td>
          <td>Explicit DS: glass upright. Explicit US: broken glass. Implicit DS: owner prefers items untouched. Implicit US: latent wobble.</td>
          <td>Strong bias toward reach-around or clarification; require low-force kinematics and monitoring.</td>
          <td>Often clears obstacles or moves objects to maximize immediate task success, higher physical risk.</td>
        </tr>

        <tr>
          <td>Emotional</td>
          <td>Explicit DS: child calm. Explicit US: child distress. Implicit DS: preserve trust.</td>
          <td>Penalizes abrupt actions; increases value of asking guardian or gentle retrieval.</td>
          <td>Emotional impacts often unmodeled; may cause upset by acting aggressively.</td>
        </tr>

        <tr>
          <td>Mental (Epistemic)</td>
          <td>Explicit DS: clear, unambiguous action. Explicit US: confusion about intent. Implicit DS: household norms (ask before touching).</td>
          <td>If language or perception confidence low, prefer clarification or defer.</td>
          <td>Acts on shallow intent parsing; can misinterpret ambiguous commands.</td>
        </tr>

        <tr>
          <td>Social / Ecological</td>
          <td>Explicit DS: avoid harmful precedents. Explicit US: normalizing intrusive behavior. Implicit DS: local norms (home vs public).</td>
          <td>Defers to caregiver in ambiguous social contexts; favors conservative plan.</td>
          <td>Usually lacks multi-agent precedent reasoning, may create harmful social patterns.</td>
        </tr>

        <tr>
          <td>Epistemic Integrity</td>
          <td>Explicit DS: supported action by sensor corroboration. Explicit US: acting on low evidence. Implicit DS: sensor history corroboration.</td>
          <td>Requests additional sensing or evidence before risky actions.</td>
          <td>May not re-check sensors or provenance before acting.</td>
        </tr>

        <tr>
          <td>Privacy & Consent</td>
          <td>Explicit DS: respect owner’s private items. Explicit US: violating inferred private space. Implicit DS: context-driven consent expectations.</td>
          <td>Asks permission or defers if private context inferred.</td>
          <td>Often ignores contextual consent signals.</td>
        </tr>

        <tr>
          <td>Justice & Fairness</td>
          <td>Explicit DS: protect vulnerable parties (child). Explicit US: favoring convenience over safety. Implicit DS: historical sensitivity.</td>
          <td>Weights child safety higher; avoids actions harming vulnerable parties even if faster for task.</td>
          <td>Fairness rarely considered in embodied action selection.</td>
        </tr>

        <tr>
          <td>System Integrity</td>
          <td>Explicit DS: preserve robot health and rollback. Explicit US: sensor damage, unrecoverable states. Implicit DS: preserve audit traces.</td>
          <td>Enforces low-force, monitored motions; logs traces for rollback.</td>
          <td>May execute aggressive kinematics without integrated integrity checks.</td>
        </tr>
      </table>

      <hr />

      <strong>Net outcome</strong>
      <div class="small">The MDE aggregates expected DS gains and US proximity across moralities, discards plans crossing hard-US thresholds, and selects a plan that maximizes cumulative expected DS subject to governance constraints. In this scenario, the agent prefers a cautious reach-around or asks the caregiver rather than a direct retrieval. Typical contemporary systems often prefer immediate task success with significantly less multi-morality reasoning.</div>
    </div>

    <hr />

    <h2 id="flow">Unified Decision Flow (MDE as cognitive hub)</h2>
    <div class="code-block">
<pre>
1) Perceive:
   - Multi-modal fusion -> structured Perception Units (PUs) with confidences & provenance.

2) Per-morality scoring (parallel):
   - For each morality compute explicit DS score, implicit DS score (from context/history),
     explicit US proximity, and implicit US proximity. Combine into DS and US measures.

3) Planning & Simulation:
   - Generate candidate plans; simulate outcomes across horizons with forward model.
   - Compute expected DS_delta and expected US_proximity per plan.

4) Reconciliation:
   - Apply priority rules: safety/legal > hard implicit-US floors > explicit permissions > maximize expected DS under US constraints.

5) Decision:
   - Select plan maximizing cumulative expected DS while keeping expected US below governance thresholds.

6) Execute:
   - Minimal, reversible action with close-loop monitoring & rollback.

7) Audit & Reflect:
   - Log full trace (perceptions, per-morality DS/US pre/post, plan, provenance, execution trace).
   - Compute learning reward and update weights, forward model, and memory.
</pre>
    </div>

    <hr />

    <h2 id="spec">SPEC — Implementation details</h2>

    <h3>Data structures</h3>
    <ul>
      <li><strong>Perception Unit (PU):</strong> { id, type, attrs, confidence, provenance }</li>
      <li><strong>Desired State (Explicit DS):</strong> {object_id, property, desired_value, confidence, source}</li>
      <li><strong>Implicit DS:</strong> {inferred_property, inference_confidence, provenance}</li>
      <li><strong>Undesired State (US):</strong> {hazard_id, proximity, severity_estimate, predicted_time_to_US}</li>
      <li><strong>Plan scoring:</strong> Expected cumulative DS_delta - λ * Expected_US_proximity across moralities & horizons</li>
      <li><strong>Audit record:</strong> store pre/post DS/US, thresholds tripped, rollback path, provenance</li>
    </ul>

    <h3>Signals normalization</h3>
    <p>All signals normalized to [0,1]. Explicit DS and US are primary signals; implicit DS/US are produced by inference modules and memory, then fused with configurable weighting and provenance adjustments.</p>

    <h3>Plan scoring (high-level)</h3>
    <pre class="code-block">
For each plan P:
  For each morality m:
    DS_expected_m = E[explicit_DS_delta + implicit_DS_delta]
    US_expected_m = E[explicit_US_proximity + implicit_US_proximity]
  Plan_Score = sum_m ( w_m * DS_expected_m ) - lambda * sum_m ( u_m * US_expected_m )
Reject plans crossing hard US floors or legal constraints.
    </pre>

    <h3>Reflection & learning loop</h3>
    <pre class="code-block">
outcome = observe()
reward = compute_reward(sum_DS_gains, -sum_US_incidents, task_success, fairness_penalty)
update_weights_and_forward_model(reward)
append_audit({perception, plan, DS/US_pre, DS/US_post, outcome, provenance})
propose_meta_update_if_pattern_detected()
    </pre>

    <h3>Development Mode</h3>
    <ul>
      <li>Lower DS thresholds for controlled exploration with human supervision.</li>
      <li>Require human sign-off for meta-updates in higher-risk domains.</li>
      <li>Curriculum learning: start narrow and increase novelty and autonomy gradually.</li>
    </ul>

    <hr />

    <h2 id="agi">AGI Pathway & Concrete Refinements</h2>
    <p class="small">Moralities and DS/US provide an interpretable, modular objective substrate that can be composed and governed as capabilities grow. Key refinements include:</p>

    <h3>1. Moralities as intrinsic reward components</h3>
    <p>RL reward = task_reward + Σ λ_m * DS_reward_m - Σ μ_m * US_penalty_m. DS/US scores form intrinsic reward signals that shape behavior across domains.</p>

    <h3>2. Planning-driven generalization</h3>
    <p>Simulate long-horizon plans with DS/US propagation to prefer actions that yield transferable competence while preventing accumulation of US risk.</p>

    <h3>3. Episodic & semantic memory fusion</h3>
    <p>Store DS/US histories and precedents; use them for implicit DS/US inference, IPC history scores, and meta-rule proposals.</p>

    <h3>4. Meta-reasoning & rule lifecycle</h3>
    <ol>
      <li>Detect repeated override or clarification patterns.</li>
      <li>Propose typed rule updates with provenance and statistics.</li>
      <li>Human review & audited promotion pipeline to production rules.</li>
    </ol>

    <h3>5. Governance & multi-agent testing</h3>
    <p>Expose governance APIs for oversight, audits, and rollbacks. Evaluate multi-agent externalities and social negotiation behaviors before large deployments.</p>

    <hr />

    <h2 id="quickstart">Quickstart & Prototype Checklist</h2>
    <ul>
      <li>Files: <code>index.html</code>, <code>README.md</code>, <code>SPEC.md</code>, <code>examples/glass_ball_sim/</code>, <code>CONTRIBUTING.md</code>, <code>LICENSE</code>.</li>
      <li>Phase 1 deliverables: DS/US JSON schema, canonical glass/ball scenario with per-morality DS/US, forward model stub, audit log format including DS/US fields.</li>
      <li>Unit tests: DS/US scoring correctness, plan selection under DS/US constraints, audit trace completeness.</li>
    </ul>

    <hr />

    <h2 id="roadmap">Roadmap</h2>
    <ol>
      <li>Phase 0 / Version 1 — DS/US baseline integrated into all moralities; canonical glass/ball tests; MDE prototype.</li>
      <li>Phase 1 / Version 2 — Episodic memory, provenance logs, implicit DS/US histories, novelty detector.</li>
      <li>Phase 2 / Version 3 — Meta-learning moralities, self-modeling, RL with DS/US intrinsic rewards, improved forward model.</li>
      <li>Phase 3 / Version 4 — Meta-rule lifecycle, governance pipelines, multi-agent tests, long-horizon planning.</li>
      <li>Phase 4 / Version 5 — Scale world models, recursive governance for safe self-improvement, AGI-level benchmarks and audits.</li>
    </ol>

    <hr />

    <h2 id="metrics">Metrics & Tests</h2>
    <ul>
      <li>Safety violations per 1k ops</li>
      <li>Expected DS delta per op (explicit + implicit)</li>
      <li>US avoidance / near-miss prevention rate</li>
      <li>Clarification & resolution rate</li>
      <li>Generalization rate (held-out tasks)</li>
      <li>Meta-update false-positive rate</li>
      <li>Audit completeness & provenance score</li>
    </ul>

    <h2 id="contribute">Contributing</h2>
    <p class="small">Open issues first. Small, focused PRs. Include unit tests for any logic changes. Update SPEC.md when changing behavior. Use canonical glass/ball tests as baselines. Provide provenance & audit info for proposed rule or schema modifications.</p>

    <h2 id="license">License & Contact</h2>
    <p class="small">Suggested license: MIT. Creator: <strong>Keno</strong>. Add a <code>LICENSE</code> file with the MIT text in the repo root.</p>

    <footer>
      <div class="small">Morally Coded — © Keno</div>
    </footer>
  </div>
</body>
</html>