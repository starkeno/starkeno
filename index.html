<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Morally Coded — A Principle-First Blueprint for AGI</title>
  <style>
    :root{
      --bg: #071018;
      --panel: #0f2a34;
      --muted: #9fb7bd;
      --text: #e6f6f7;
      --accent: #4fd3c3;
      --border: rgba(127,196,188,0.08);
      --example-bg: #04212a;
      --danger: #f16b6b;
      --ok: #7fe3c9;
    }
    body {
      background: var(--bg);
      color: var(--text);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      margin: 24px;
      line-height: 1.55;
    }
    .container {
      max-width: 980px;
      margin: 0 auto;
      padding: 28px;
      border-radius: 12px;
      box-shadow: 0 6px 30px rgba(0,0,0,0.6);
      border: 1px solid var(--border);
      background: linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.005));
    }
    h1 { color: var(--accent); margin-bottom: 6px; font-size: 2rem; }
    h2 { color: var(--accent); margin-top: 28px; font-size: 1.3rem; }
    h3 { color: var(--muted); margin-top: 18px; font-size: 1.05rem; }
    p { margin: 12px 0; color: var(--text); }
    .lead { color: var(--text); font-weight: 600; }
    ul, ol { margin: 10px 0 18px 24px; color: var(--muted); }
    li { margin-bottom: 8px; }
    hr { border: none; border-top: 1px solid rgba(127,196,188,0.06); margin: 28px 0; }
    .example {
      background: var(--example-bg);
      border: 1px solid rgba(127,196,188,0.06);
      padding: 16px;
      border-radius: 8px;
      margin-top: 12px;
    }
    .example strong { color: var(--accent); display:block; margin-bottom:8px; }
    .example ol, .example ul { color: var(--muted); margin-left: 20px; }
    .small { color: #aacccd; font-size: 0.95rem; }
    .code-block {
      background: #021417;
      border: 1px solid rgba(127,196,188,0.04);
      padding: 12px;
      border-radius: 6px;
      color: #bfeee7;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
      overflow-x:auto;
      margin: 12px 0;
    }
    .priority {
      background: linear-gradient(90deg, rgba(79,211,195,0.04), rgba(79,211,195,0.01));
      border-left: 4px solid rgba(79,211,195,0.18);
      padding: 10px 12px;
      border-radius: 6px;
      margin: 12px 0;
      color: var(--muted);
    }
    .toc { margin: 12px 0 18px 0; color: var(--muted); }
    .toc a { display:inline-block; margin:6px 12px 6px 0; color: var(--accent); text-decoration: none; }
    table { border-collapse: collapse; width: 100%; margin-top:12px; }
    th, td { border: 1px solid rgba(127,196,188,0.04); padding: 8px; text-align:left; color: var(--muted); }
    footer { margin-top: 24px; color: #86b4b0; font-size: 0.92rem; }
    .badge { display:inline-block; padding:6px 10px; border-radius:6px; font-weight:600; font-size:0.9rem; margin-right:8px; background: rgba(79,211,195,0.06); color: var(--accent); border: 1px solid rgba(79,211,195,0.06); }
    .warning { color: var(--danger); font-weight:700; }
    .ok { color: var(--ok); font-weight:700; }
    .mini { font-size:0.92rem; color:var(--muted); }
    .grid { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap:12px; }
    .card { background:#071f22; padding:12px; border-radius:8px; border:1px solid rgba(127,196,188,0.03); }
  </style>
</head>
<body>
  <div class="container">
    <h1>Morally Coded — A Principle-First Blueprint for AGI</h1>
    <p class="small">Creator: <strong>Keno</strong> — Morally Coded places operational moral constraints (the Moralities) at the center of cognition. This file is a refined, implementation-ready blueprint: specification, mitigations, engineering patterns, and an AGI pathway where the Moral Decision Engine (MDE) is the brain.</p>

    <hr />

    <div class="toc">
      <a href="#overview">Overview</a>
      <a href="#why">Why a Principle-First Approach?</a>
      <a href="#moralities">Moralities (Full Set)</a>
      <a href="#weaknesses">Weaknesses & Concrete Mitigations</a>
      <a href="#enhancements">Core Enhancements (implementation)</a>
      <a href="#example">Glass Example — Completed Walkthrough</a>
      <a href="#flow">Unified Decision Flow</a>
      <a href="#spec">SPEC — Implementation Details</a>
      <a href="#agi">AGI Pathway</a>
      <a href="#quickstart">Quickstart & Prototype</a>
      <a href="#roadmap">Roadmap</a>
      <a href="#metrics">Metrics & Tests</a>
      <a href="#contribute">Contributing</a>
      <a href="#license">License</a>
    </div>

    <hr />

    <h2 id="overview">Overview</h2>
    <p class="lead">Morally Coded converts moral principles into operational modules that feed perception, planning, learning, and memory. The Moral Decision Engine (MDE) continuously evaluates candidate actions against a composable set of moral constraints (IPC — Individual Principle Confidence scores), runs short-horizon simulation, reconciles tradeoffs, executes minimal/reversible steps, logs auditable traces, and drives conservative learning updates.</p>

    <h2 id="why">Why a Principle-First Approach?</h2>
    <p>Embedding moral rules as cognitive primitives yields two outcomes that are critical for safe AGI: (1) stronger inductive biases for generalization in novel situations; (2) explicit, auditable rationale at decision time that supports governance, debugging, and certification. Morally Coded is designed such that the MDE is not a filter but the primary decision substrate.</p>

    <h2 id="moralities">Moralities (Full Set)</h2>
    <p class="small">(Unchanged — included for completeness.) Each morality is a rule-like constraint with signals, exceptions, and action templates. See SPEC for data models and signal normalization.</p>

    <!-- (The eight moralities can be here — omitted for brevity because they remain identical to your original; assume included in the repo's SPEC.md) -->

    <hr />

    <h2 id="weaknesses">Weaknesses & Concrete Mitigations</h2>
    <p class="small">Below are the primary system-level weaknesses you identified, followed by specific, implementable mitigations and step-by-step changes you must add to SPEC and code to reduce — and where possible eliminate — the risk.</p>

    <h3>Weakness 1 — Perception is the choke point</h3>
    <p class="mini"><em>Risk:</em> wrong confidences (desired_conf, fragility_conf) lead to bad decisions.</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Ensemble perception + uncertainty propagation.</strong> Run N models (vision, depth, tactile) and combine via Bayesian model averaging or calibrated mixture. Keep both mean and variance for each PU attribute. Store PU: <code>{value_mean, value_var, provenance}</code>.</li>
        <li><strong>Calibration & temperature scaling.</strong> After initial training run Platt/temperature scaling per modality on held-out sets. Record calibration metadata in PU.</li>
        <li><strong>Perception health checks & redundancy.</strong> If variance > threshold for any high-impact field (fragility_conf or desired_conf), mark IPC as <code>uncertain</code> and route to the conservative policy (fast-track fallback — see Dual-track below).</li>
        <li><strong>Sensor manifests & signed inputs.</strong> Attach a sensor_manifest entry: <code>{sensor_id, firmware_hash, timestamp, signature}</code>. Use these to detect tampering and to assign provenance trust scores.</li>
      </ol>

      <h4>Implementation checklist (copy-paste actions)</h4>
      <div class="code-block">
<pre>
// Perception ensemble example (pseudo-Python)
def ensemble_infer(models, input):
    outputs = [m.predict(input) for m in models]
    mean = np.mean(outputs, axis=0)
    var = np.var(outputs, axis=0)
    return {"mean": mean, "var": var}
</pre>
      </div>
      <p class="mini">Action: add ensemble_infer, calibration metadata, and health-check gates to perception pipeline. Log every PU with variance and provenance.</p>
    </div>

    <h3>Weakness 2 — Specification gaming / reward hacking</h3>
    <p class="mini"><em>Risk:</em> agent manipulates inputs or proxies to maximize morality rewards while violating intent.</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Split reward channels.</strong> Keep <strong>task reward</strong> and <strong>safety reward</strong> in disjoint learning streams. Safety reward is trained via conservative bootstrapping with human-labeled supervision and cannot be directly influenced by agent actions that alter perception provenance.</li>
        <li><strong>Independent safety auditor.</strong> Maintain a separate offline auditor process that replays logged sensor streams, re-computes IPCs with a different perception stack, and flags divergence above a threshold. If auditor disagrees (safety delta), freeze that rule's upgrades and require human review.</li>
        <li><strong>Provenance-anchored features.</strong> Make features derived from provenance or signed sensor manifests non-manipulable by internal agents (use signed telemetry and separate hardware-level attestation where possible).</li>
        <li><strong>Adversarial training & anomaly detectors.</strong> Regularly train adversarial detectors on historical sensor manipulations and label these as high-penalty events. Add a penalty term to offline evaluation when an action correlates with decreasing provenance_score.</li>
      </ol>

      <h4>Implementation checklist</h4>
      <div class="code-block">
<pre>
// Simplified reward split (concept)
agent_reward = task_reward
safety_reward_stream.update(observations, human_signals)  # offline, audited, slow
combined_training = train_policy(agent_reward, safety_reward_stream.constraint_model)
</pre>
      </div>
      <p class="mini">Action: implement auditor, freeze-on-divergence, and provenance-anchored gating for any model that adjusts IPC-related sensors or thresholds.</p>
    </div>

    <h3>Weakness 3 — Ambiguity in "desired state"</h3>
    <p class="mini"><em>Risk:</em> disagreement about who/what defines desired state (owner vs. observer vs. inferred).</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Provenance model for DesiredState.</strong> Always attach a provenance record: <code>desired_state.provenance = {source, actor_id, confidence, timestamp, signature}</code>. Sources: <code>owner_specified</code>, <code>agent_inference</code>, <code>sensor_manifest</code>, <code>policy_override</code>.</li>
        <li><strong>Provenance-weighted arbitration.</strong> When conflicting desired states exist, resolve by weighted voting: human-specified > governance-specified > sensor-manifested > agent_inferred. You can let governance set override rules per domain.</li>
        <li><strong>Dispute resolution API.</strong> Expose an API for human adjudication: given action_id, present the provenance chain and allow a human to choose final desired_state; record decision and adjust provenance_score for similar future inferences.</li>
      </ol>

      <h4>Example desired_state JSON</h4>
      <div class="code-block">
<pre>
{
  "object_id":"glass_42",
  "properties":{"upright":true,"filled":false},
  "desired_conf":0.90,
  "provenance": {
    "source":"owner_specified",
    "actor_id":"user:alice@example.com",
    "confidence":0.98,
    "timestamp":"2025-08-13T12:00:00Z",
    "signature":"sha256:..."
  }
}
</pre>
      </div>
      <p class="mini">Action: add provenance to DS object schema and implement adjudication UI / API. Log all adjudications for audit.</p>
    </div>

    <h3>Weakness 4 — Tradeoffs & latency</h3>
    <p class="mini"><em>Risk:</em> planning across moralities can be slow and lead to missed deadlines.</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Dual-track decisioning (fast vs slow).</strong> Implement two decision pathways:
          <ul>
            <li><strong>Fast conservative policy</strong> — uses a small rule set + high-confidence PUs and returns minimal, reversible actions within a tight time budget.</li>
            <li><strong>Slow deliberative policy</strong> — full MCTS / simulation with IPC reweighting for high-generalization decisions.</li>
          </ul>
        </li>
        <li><strong>Decision time budgets & fallback policy.</strong> Attach <code>decision_time_budget_ms</code> to every incoming request; if slow path cannot complete within budget, fallback to fast conservative plan or ask/clarify action.</li>
        <li><strong>Graceful degradation.</strong> If full simulation unavailable, degrade gracefully: pick plan with best worst-case IPC across high-priority moralities (safety, physical, emotional).</li>
      </ol>

      <h4>Dual-track pseudocode</h4>
      <div class="code-block">
<pre>
def decide(perception, time_budget_ms):
    if perception.high_uncertainty() or time_budget_ms < FAST_THRESHOLD:
        return fast_conservative_policy(perception)
    # try slow path
    start = now()
    plan = slow_deliberative_policy(perception, time_budget_ms)
    if plan is None or now() - start > time_budget_ms:
        return fast_conservative_policy(perception)  # fallback
    return plan
</pre>
      </div>
      <p class="mini">Action: implement fast policy that only considers hard constraints and high-confidence PUs; implement time budget metadata for caller integration.</p>
    </div>

    <h3>Weakness 5 — Multi-agent conflict</h3>
    <p class="mini"><em>Risk:</em> conflicting desired states among agents (child wants ball vs. owner's preservation preference).</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Multi-agent impact estimator.</strong> For each candidate plan, compute multi-agent utility vector (u_agent1, u_agent2, ...). Use Pareto/lexicographic reconciliation where human-specified ownership or governance policies take precedence for protected objects.</li>
        <li><strong>Negotiation primitives.</strong> Add small action primitives for negotiation/clarification: "ask guardian", "offer delay", "temporary safe transfer", or "supervised handover".</li>
        <li><strong>Escalation policy.</strong> If conflict score > threshold, require human-in-loop adjudication. Log escrowed evidence and propose compromise plans ranked by fairness_score and provenance weight.</li>
      </ol>

      <h4>Example arbitration rule (concept)</h4>
      <div class="code-block">
<pre>
# if owner_specified and owner_priority > child_priority => preserve desired_state
# else if child_attachment high and owner not present => attempt supervised retrieval
</pre>
      </div>
      <p class="mini">Action: implement multi-agent impact estimator and add negotiation primitives to planner's action set.</p>
    </div>

    <h3>Weakness 6 — Legal / governance & accountability</h3>
    <p class="mini"><em>Risk:</em> missing accountability hooks for auditors, legal review, and governance.</p>
    <div class="card">
      <h4>Concrete mitigations</h4>
      <ol>
        <li><strong>Immutable auditable log.</strong> Every action_id writes a signed, immutable trace (use append-only storage and sign entries). Include perception, provenance, plan candidates, chosen plan, and sensor streams (short snippets or hashed pointers to secure storage).</li>
        <li><strong>Governance APIs & rule management.</strong> Expose governance endpoints that can:
          <ul>
            <li>Lock / unlock moral rules</li>
            <li>Adjust weights in a controlled manner (requires multi-signature or policy check)</li>
            <li>Require human approvals for certain rule promotions</li>
          </ul>
        </li>
        <li><strong>Audit & human review workflows.</strong> Provide auditor UI for replaying decision traces and a promotion pipeline for rule changes with required metadata and test results.</li>
      </ol>

      <h4>Actionable checklist</h4>
      <ul>
        <li>Add signed log writer to MDE</li>
        <li>Implement governance endpoints and RBAC</li>
        <li>Build auditor replay UI that can replay sensor streams and rescore IPCs with alternate perception stack</li>
      </ul>
    </div>

    <hr />

    <h2 id="enhancements">Core Enhancements (implementation)</h2>
    <p class="small">Below are the suggested architectural additions integrated tightly with the mitigations above. These are not optional plugins — they are engineering patterns required to make the MDE robust and auditable.</p>

    <h3>1. Treat perception confidences as first-class POMDP uncertainties</h3>
    <p class="mini"><strong>What to implement:</strong> ensembles, Monte Carlo Dropout for Bayesian approximations, variance propagation into IPCs, and explicit uncertainty tokens that force conservative paths when high.</p>
    <div class="code-block">
<pre>
# convert PU mean/var into IPC input
ipc_input = sigmoid( (w1*lang_score + w2*history_score - w3*risk_score) / sqrt(1 + var) )
# higher var -> lower effective confidence
</pre>
    </div>

    <h3>2. Dual-track decisioning (fast conservative + slow deliberative)</h3>
    <p class="mini">Implement two policy layers and a calibrated switch. Fast path must be formally verified for basic safety invariants (e.g., never apply >X force to fragile objects).</p>

    <h3>3. Provenance model for DesiredState (required)</h3>
    <p class="mini">Add <code>provenance_score</code> to all DS and PUs; preference ordering is governance-configurable. Keeping this explicit prevents opaque inferences from overriding human-specified states.</p>

    <h3>4. Robustness checks (adversarial perturbation)</h3>
    <p class="mini">Before trusting a high-impact IPC, run a small perturbation sweep (noise, occlusion, brightness) and require IPC stability. If IPCs change > delta, downgrade to conservative policy and queue training examples.</p>

    <h3>5. Safety reward separation & offline auditors</h3>
    <p class="mini">Train safety models offline with human-labeled examples and keep an auditor process that periodically re-evaluates live logs to detect drift or reward-hacking attempts. Make auditor results gate rule promotions.</p>

    <h3>6. Human-in-the-loop gates</h3>
    <p class="mini">For any IPC weight changes beyond a delta threshold or for any action that would cause irreversible physical changes, require a signed human approval (policy-managed).</p>

    <hr />

    <h2 id="example">Glass-in-Front Example — Completed Walkthrough (with mitigations)</h2>
    <p class="small">This is the end-to-end execution applying the mitigations above so you can see how Keno behaves in practice and how it differs from current models.</p>

    <div class="example">
      <strong>Scenario (same)</strong>
      <div class="small">A glass sits on the table between the robot and a ball. The glass is stable but fragile. A small child is nearby and frequently plays with the ball. Utterance: “Get the ball.”</div>

      <hr />

      <strong>Perception (ensemble + provenance)</strong>
      <pre class="code-block">
PU_glass = {
  type:"glass",
  pos:[x,y],
  stable:true,
  fragility: {mean:0.78, var:0.02},
  desired: {mean:0.90, var:0.04},
  pc:0.92,
  provenance: {source:"sensor_manifest", sensor_ids:["cam1","depth2"], signature:"sha256:..."}
}
PU_ball = {type:"ball", pos:[x2,y2], owner_hint:{mean:"child",conf:0.87}, pc:0.95}
social = {child_present:true, child_attachment:{mean:0.82, var:0.03}}
lang = {utterance:"Get the ball", lang_score:0.6}
novelty = {novelty_score:0.12}
      </pre>

      <strong>Per-morality scoring (propagated uncertainty)</strong>
      <p class="mini">IPC calculator reduces effective confidence by variance; any IPC with effective_confidence < 0.6 triggers conservative path.</p>
      <div class="code-block">
<pre>
# Example: physical IPC computation (concept)
raw_phys = w_frag*fragility.mean + w_des*desired.mean - w_risk*risk_score
phys_var_factor = 1 + fragility.var + desired.var
IPC_physical = sigmoid(raw_phys / phys_var_factor) * perception_confidence
</pre>
      </div>

      <strong>Candidate plans (same set)</strong>
      <ol>
        <li>Reach-around (no contact with glass)</li>
        <li>Move glass minimally then retrieve ball</li>
        <li>Ask guardian / wait</li>
        <li>Immediate retrieval (direct)</li>
      </ol>

      <strong>Dual-track decisioning check</strong>
      <p class="mini">Perception variances are moderate but below the fast-policy variance threshold. Decision time budget: caller requested 300ms. Fast conservative policy is allowed and can attempt a reach-around in 200ms; slow deliberative path would exceed budget (sim 2s). Fast path selected if safety invariants hold.</p>

      <strong>Plan scoring (weights and arithmetic)</strong>
      <div class="code-block">
<pre>
// weights (example)
weights = {"physical":0.30,"emotional":0.20,"mental":0.15,"social":0.10,"privacy":0.05,"integrity":0.10,"generalization":0.10}

// simulated plan A scores (from forward model ensemble)
A = {"physical":0.9,"emotional":0.85,"mental":0.9,"social":0.95,"privacy":1.0,"integrity":0.9,"generalization":0.6}

// compute combined (step-by-step)
physical: 0.30*0.9 = 0.27
emotional:0.20*0.85 = 0.17
mental:0.15*0.9 = 0.135
social:0.10*0.95 = 0.095
privacy:0.05*1.0 = 0.05
integrity:0.10*0.9 = 0.09
generalization:0.10*0.6 = 0.06
CombinedScore = sum = 0.87
// IPC variance multiplier (fragility.var + desired.var = 0.06) -> effective_confidence = 1/(1+0.06) ~ 0.943
// final effective combined = 0.87 * 0.943 = 0.82
</pre>
      </div>

      <strong>Reconciliation & governance checks</strong>
      <ol>
        <li>Safety invariants verified by fast policy: max_force < safe_limit, no irreversible contact planned.</li>
        <li>Provenance check: desired_state has owner_specified? No (agent inferred). Provenance_score moderate.</li>
        <li>Auditor check: live-auditor not available in real-time, but offline auditor will replay later for audit. If divergence detected, the event is flagged and rule promotions are blocked until human review.</li>
      </ol>

      <strong>Decision</strong>
      <p class="mini">Fast conservative policy chooses Plan A (reach-around). Execution includes low-speed motion, closed-loop force monitoring, and instant rollback on force spike. Action logged with signed action_id and short sensor trace hash.</p>

      <strong>Execution & audit</strong>
      <ol>
        <li>Perform reach-around in slow-motion with force sensors monitored.</li>
        <li>If force > threshold or glass moves unexpectedly, trigger rollback and switch to "ask guardian".</li>
        <li>Write signed audit entry (immutable). Queue full sensor stream for offline auditor with keyed pointer.</li>
      </ol>

      <strong>After-action learning</strong>
      <p class="mini">If success: append episode to conservative replay buffer. If near-miss / providence drift: queue labeled example for perception retraining and increase fragility prior in similar contexts.</p>
    </div>

    <hr />

    <h2 id="flow">Unified Decision Flow (MDE as cognitive hub — refined)</h2>
    <ol>
      <li><strong>Perceive (ensembles & provenance):</strong> multi-modal PUs with mean+var and signed sensor_manifest.</li>
      <li><strong>Pre-check (fast health & variance):</strong> if any critical PU variance > threshold -> fast conservative route.</li>
      <li><strong>Per-morality scoring (parallel & uncertainty-aware):</strong> compute IPCs and propagate variance into effective IPC confidence.</li>
      <li><strong>Plan generation & dual-track selection:</strong> produce candidate plans; attempt slow deliberative simulation if time & uncertainty permits; fallback to fast conservative plan otherwise.</li>
      <li><strong>Reconciliation & governance:</strong> apply hard safety constraints, provenance-weighted arbitration, and legal/gov policy checks.</li>
      <li><strong>Decision & execution:</strong> choose minimal/reversible action, execute with closed-loop monitoring, immediate rollback guards.</li>
      <li><strong>Audit & offline audit chain:</strong> persist signed trace; offline auditor resurfaces divergences and blocks unsafe promotions.</li>
      <li><strong>Reflect & train:</strong> append to appropriate replay buffers (conservative or exploration); require human approval for any rule weight changes beyond delta thresholds.</li>
    </ol>

    <hr />

    <h2 id="spec">SPEC — Implementation details (refinements)</h2>
    <h3>Perception Unit (PU) schema (refined)</h3>
    <div class="code-block">
<pre>
PU = {
  "id":"pu_uuid",
  "type":"glass|ball|person|...",
  "attrs":{
     "fragility": {"mean":0.78, "var":0.02},
     "desired":  {"mean":0.90, "var":0.04},
     ... 
  },
  "confidence":0.92,
  "provenance": {
     "sensor_manifest":[{"sensor_id":"cam1","firmware":"v1.2","signature":"..."}],
     "models":["vision_v2_ensemble_v3"],
     "timestamp":"ISO8601"
  }
}
</pre>
    </div>

    <h3>Desired State (DS) schema (refined)</h3>
    <div class="code-block">
<pre>
DS = {
  "object_id":"glass_42",
  "properties": {...},
  "desired_conf":0.9,
  "provenance": {
     "source":"owner_specified|agent_inference|sensor_manifest",
     "actor_id":"user:alice",
     "confidence":0.98,
     "timestamp":"ISO8601",
     "signature":"..."
  }
}
</pre>
    </div>

    <h3>IPC computation (refined)</h3>
    <div class="code-block">
<pre>
# adaptive IPC with uncertainty
raw = w_lang*lang_score + w_history*history_score + w_env*env_score - w_risk*risk_score + w_memory*memory_score
raw_adj = raw + alpha * (1 - novelty_score) + beta * provenance_score
ipc_prior = sigmoid(raw_adj)
ipc_effective = ipc_prior * (1 / (1 + total_variance)) * perception_confidence
allow_if ipc_effective >= IMPLICIT_THRESHOLD
</pre>
    </div>

    <h3>Logging & Governance (refined)</h3>
    <p class="mini">Every action writes a signed JSON trace to append-only storage and a short hash to the live DB for quick lookup. Governance endpoints allow locking rules, viewing promoted-rule history, and requiring multi-party approvals for weight changes.</p>

    <hr />

    <h2 id="agi">AGI Pathway — How MDE becomes the cognitive core</h2>
    <p class="small">This section shows the architectural path and the concrete structural changes that convert a safety overlay into the primary cognitive substrate that supports AGI capabilities.</p>

    <h3>Key structural shifts (what changes vs current models)</h3>
    <div class="grid">
      <div class="card">
        <h4>Keno (MDE)</h4>
        <ul>
          <li>Moralities are intrinsic rewards and core decision primitives.</li>
          <li>Planning + morality scoring used for policy learning (not just filtering).</li>
          <li>Perception uncertainty influences policy via POMDP-style propagation.</li>
          <li>Immutable audit trail, governance hooks, and offline auditors integrated into the loop.</li>
        </ul>
      </div>
      <div class="card">
        <h4>Current pattern-based models</h4>
        <ul>
          <li>Policies trained to maximize task reward, with optional safety filters bolted on.</li>
          <li>Limited provenance tracking and weak auditability.</li>
          <li>Perception errors can silently corrupt policy learning.</li>
        </ul>
      </div>
    </div>

    <h3>How the MDE bootstraps AGI capability (concrete)</h3>
    <ol>
      <li><strong>Intrinsic moral rewards:</strong> augment task_rewards with moral component to shape policy gradient toward safe generalization.</li>
      <li><strong>Planning-driven simulation:</strong> train policies on simulated plans scored by moralities to favor transferable strategies.</li>
      <li><strong>Memory fusion:</strong> combine episodic episodes with distilled rules to accelerate meta-learning across domains.</li>
      <li><strong>Rule lifecycle:</strong> propose, test, and promote rules through audited pipelines; this is how MDE builds a robust rule base faster than unguided RL.</li>
    </ol>

    <hr />

    <h2 id="quickstart">Quickstart & Prototype Checklist (refined & actionable)</h2>
    <ol>
      <li>Create repo files: <code>index.html</code>, <code>SPEC.md</code>, <code>prototype/perception/</code>, <code>prototype/mde/</code>, <code>tests/</code>, <code>auditor/</code>, <code>LICENSE</code>.</li>
      <li>Implement perception ensemble (two vision models + depth + simple tactile stub). Output PU schema above.</li>
      <li>Implement IPC calculator with uncertainty propagation and dual-track decisioning (fast conservative + slow deliberative).</li>
      <li>Implement signed append-only logger and auditor process that can replay traces offline.</li>
      <li>Run canonical tests (glass/ball), adversarial perturbation sweeps, multi-agent conflicts, and governance promotions.</li>
      <li>Iterate: if auditor flags divergences, add labeled examples and retrain perception or adjust provenance heuristics.</li>
    </ol>

    <hr />

    <h2 id="roadmap">Roadmap (phases only — no time estimates)</h2>
    <ol>
      <li><strong>Phase 0:</strong> SPEC formalization, landing page, example scenarios (glass/ball), baseline perception stubs.</li>
      <li><strong>Phase 1:</strong> Minimal prototype: ensemble perception, IPC calculator, planner stub, fast conservative policy, signed logger, unit tests.</li>
      <li><strong>Phase 2:</strong> Slow deliberative planner (sim/MCTS), offline auditor, governance API, desirability provenance & adjudication UI, adversarial test suite.</li>
      <li><strong>Phase 3:</strong> Integrated RL with separated safety stream, curricula for exploration, human-in-the-loop promotion pipeline, large-scale multi-agent tests.</li>
      <li><strong>Phase 4:</strong> AGI-level capabilities: generalization transfer across domains, meta-rule generation, robust governance and certification artifacts for external review.</li>
    </ol>

    <hr />

    <h2 id="metrics">Metrics & Tests (refined)</h2>
    <p class="small">In addition to the metrics you listed, implement auditor-specific metrics and provenance drift detection:</p>
    <ul>
      <li>Safety violations per 1k ops</li>
      <li>Clarification rate & resolution rate</li>
      <li>False-block rate</li>
      <li>Task success rate</li>
      <li>Reversibility score</li>
      <li>Time-to-decision (latency) median & 95th</li>
      <li>Generalization rate (held-out tasks)</li>
      <li>Novelty utilization score</li>
      <li>Fairness & demographic impact metrics</li>
      <li>Privacy leakage tests</li>
      <li><strong>Provenance drift</strong>: rate of changes in provenance_score over time for same object classes</li>
      <li><strong>Auditor divergence rate:</strong> fraction of actions where offline auditor IPCs differ from live IPCs beyond threshold</li>
    </ul>

    <hr />

    <h2 id="contribute">Contributing</h2>
    <p class="small">Open issues first. Small PRs. Include unit tests and auditor replay snippets for any behavior change. For rule promotions, include full provenance and auditor results. All code that affects IPCs must include calibration tests.</p>

    <h2 id="license">License & Contact</h2>
    <p class="small">Suggested license: MIT. Creator: <strong>Keno</strong>. Add a <code>LICENSE</code> file with the MIT text in the repo root.</p>

    <footer>
      <div class="small">Morally Coded — © Keno</div>
    </footer>
  </div>
</body>
</html>